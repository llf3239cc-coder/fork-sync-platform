#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Classify PRs in filtered_prs_split (merged PRs only).

Features:
1. Read PR data from filtered_prs_split (generated by filter_prs_from_forks_with_unique_commits.py)
2. Filter out closed PRs; process only merged PRs
3. Call DeepSeek API to classify PRs (prompt from pr_classification_prompt.txt)
4. Analysis includes: PR title, body, and related commit messages
5. Save results to JSON and CSV
6. Concurrent requests, retry on failure, checkpoint save
"""

import os
import time
import json
import asyncio
import csv
import argparse
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from asyncio import Semaphore
import httpx
from asyncio import as_completed

# Windows: msvcrt; Unix/Linux/Mac: fcntl
if sys.platform == "win32":
    try:
        import msvcrt
    except ImportError:
        msvcrt = None
    fcntl = None
else:
    msvcrt = None
    try:
        import fcntl
    except ImportError:
        fcntl = None

# ===================== Config =====================
CURRENT_DIR = Path(os.path.dirname(os.path.abspath(__file__)))

INPUT_DIR = CURRENT_DIR / "filtered_prs_split"
PROMPT_FILE = CURRENT_DIR / "pr_classification_prompt.txt"

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "sk-ea182b22c22d403f9d093aaf588c6f98")
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1/chat/completions"

OUTPUT_DIR = CURRENT_DIR / "pr_classification_results"
LOG_FILE = CURRENT_DIR / "pr_classification_log.txt"
CATEGORIES_FILE = CURRENT_DIR / "pr_classification_categories.json"
STATUS_FILE = CURRENT_DIR / "pr_classification_status.json"

CONCURRENT_REQUESTS = 20
RETRY_TIMES = 3
RETRY_DELAY = 1
API_TIMEOUT = 45

# ===================== Helpers =====================
def log_record(msg: str):
    """Log message to file and console."""
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\n")
    try:
        print(msg)
    except UnicodeEncodeError:
        import re
        safe_msg = re.sub(r'[^\x00-\x7F]+', '', msg)
        print(safe_msg)


def load_categories() -> List[str]:
    """Load category list (file lock for multi-process read)."""
    if not CATEGORIES_FILE.exists():
        initial_categories = [
            "Bug Fix",
            "Compatibility Fix",
            "Performance Optimization",
            "Code Refactoring",
            "Documentation Update",
            "Project-Specific Modification",
            "Experimental Feature",
            "Test Related",
            "Build / CI Related"
        ]
        save_categories(initial_categories)
        return initial_categories
    
    while True:
        try:
            with CATEGORIES_FILE.open("r", encoding="utf-8") as f:
                if sys.platform != "win32" and fcntl:
                    fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                
                try:
                    data = json.load(f)
                    categories = data.get("categories", [])
                    if categories:
                        return categories
                finally:
                    if sys.platform != "win32" and fcntl:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                        except Exception:
                            pass
        except json.JSONDecodeError:
            time.sleep(0.2)
            continue
        except Exception as e:
            log_record(f"Warning: load categories failed: {e}, retrying...")
            time.sleep(0.2)
            continue

    return [
        "Bug Fix",
        "Compatibility Fix",
        "Performance Optimization",
        "Code Refactoring",
        "Documentation Update",
        "Project-Specific Modification",
        "Experimental Feature",
        "Test Related",
        "Build / CI Related"
    ]


def save_categories(categories: List[str]):
    """Save category list (file lock for multi-process write; atomic write)."""
    lock_file = CATEGORIES_FILE.with_suffix(".lock")
    wait_interval = 0.1
    lock_fd = None
    
    try:
        lock_file.parent.mkdir(parents=True, exist_ok=True)
        lock_acquired = False

        while not lock_acquired:
            try:
                if sys.platform == "win32" and msvcrt:
                    lock_fd = lock_file.open("wb+")
                    try:
                        msvcrt.locking(lock_fd.fileno(), msvcrt.LK_NBLCK, 1)
                        lock_acquired = True
                    except (IOError, OSError):
                        lock_fd.close()
                        lock_fd = None
                        time.sleep(wait_interval)
                        continue
                else:
                    lock_fd = lock_file.open("w")
                    try:
                        fcntl.flock(lock_fd.fileno(), fcntl.LOCK_EX)
                        lock_acquired = True
                    except (IOError, BlockingIOError, OSError):
                        lock_fd.close()
                        lock_fd = None
                        time.sleep(wait_interval)
                        continue
            except Exception:
                if lock_fd:
                    try:
                        lock_fd.close()
                    except Exception:
                        pass
                    lock_fd = None
                time.sleep(wait_interval)
                continue
        
        try:
            current_categories = load_categories_unlocked()
            merged_categories = list(dict.fromkeys(current_categories + categories))
            temp_file = CATEGORIES_FILE.with_suffix(".tmp")
            with temp_file.open("w", encoding="utf-8") as f:
                json.dump({
                    "categories": merged_categories,
                    "updated_at": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            temp_file.replace(CATEGORIES_FILE)
        finally:
            if lock_fd:
                try:
                    if sys.platform == "win32" and msvcrt:
                        msvcrt.locking(lock_fd.fileno(), msvcrt.LK_UNLCK, 1)
                    else:
                        fcntl.flock(lock_fd.fileno(), fcntl.LOCK_UN)
                    lock_fd.close()
                except Exception:
                    pass
            
            try:
                if lock_file.exists():
                    lock_file.unlink()
            except Exception:
                pass

    except Exception as e:
        log_record(f"Warning: save categories failed: {e}")
        if lock_fd:
            try:
                lock_fd.close()
            except Exception:
                pass


def load_categories_unlocked() -> List[str]:
    """Load categories without lock (for use inside save_categories)."""
    if not CATEGORIES_FILE.exists():
        return [
            "Bug Fix",
            "Compatibility Fix",
            "Performance Optimization",
            "Code Refactoring",
            "Documentation Update",
            "Project-Specific Modification",
            "Experimental Feature",
            "Test Related",
            "Build / CI Related"
        ]
    
    try:
        with CATEGORIES_FILE.open("r", encoding="utf-8") as f:
            data = json.load(f)
            categories = data.get("categories", [])
            if categories:
                return categories
    except Exception:
        pass
    
    return [
        "Bug Fix",
        "Compatibility Fix",
        "Performance Optimization",
        "Code Refactoring",
        "Documentation Update",
        "Project-Specific Modification",
        "Experimental Feature",
        "Test Related",
        "Build / CI Related"
    ]


def add_category_if_new(category: str, existing_categories: List[str]) -> List[str]:
    """If category is new, add to list and return updated list."""
    if not category:
        return existing_categories

    predefined = [
        "Bug Fix",
        "Compatibility Fix",
        "Performance Optimization",
        "Code Refactoring",
        "Documentation Update",
        "Project-Specific Modification",
        "Experimental Feature",
        "Test Related",
        "Build / CI Related"
    ]
    
    if category in predefined:
        return existing_categories

    if category not in existing_categories:
        updated_categories = existing_categories + [category]
        save_categories(updated_categories)
        log_record(f"New category: {category}, added (total {len(updated_categories)} categories)")
        return updated_categories
    
    return existing_categories


def build_prompt_with_categories(base_prompt: str, categories: List[str]) -> str:
    """Build prompt with dynamic category list."""
    categories_text = "\n".join([f"- {cat}" for cat in categories])
    prompt = base_prompt.replace("[CATEGORIES_LIST]", categories_text)
    
    return prompt


def load_prompt() -> str:
    """Load classification prompt."""
    if not PROMPT_FILE.exists():
        log_record(f"Prompt file not found: {PROMPT_FILE}")
        return ""
    
    with PROMPT_FILE.open("r", encoding="utf-8") as f:
        return f.read().strip()


def sanitize_filename(name: str) -> str:
    """Sanitize filename (remove invalid chars)."""
    import re
    name = name.replace("/", "__")
    name = re.sub(r'[<>:"|?*]', "_", name)
    return name


def load_prs_from_directory_batch(
    input_dir: Path,
    output_dir: Path,
    batch_size: int = 1000
):
    """Load PRs from filtered_prs_split in batches (generator). Yields (PR list, stats)."""
    if not input_dir.exists() or not input_dir.is_dir():
        log_record(f"Directory not found: {input_dir}")
        return

    log_record(f"Loading PRs from: {input_dir}")
    log_record("Scanning input directory...")

    batch_prs = []
    total_files = 0
    closed_count = 0
    already_processed_count = 0
    total_valid_prs = 0

    existing_output_files = get_existing_output_files(output_dir)

    try:
        for repo_dir in input_dir.iterdir():
            if not repo_dir.is_dir():
                continue

            repo_dir_name = repo_dir.name

            for pr_file in repo_dir.glob("*__PR_*.json"):
                total_files += 1
                try:
                    with pr_file.open("r", encoding="utf-8") as f:
                        pr_data = json.load(f)
                    
                    repo_full_name = pr_data.get("repo_full_name", "unknown")
                    pr = pr_data.get("pr", {})
                    
                    if not pr:
                        continue
                    
                    pr_status = pr.get("pr_status", "")
                    if pr_status == "closed":
                        closed_count += 1
                        continue

                    output_file = output_dir / repo_dir_name / pr_file.name
                    output_relative = str(output_file.relative_to(output_dir))
                    if output_relative in existing_output_files:
                        already_processed_count += 1
                        continue

                    if "repo_full_name" not in pr:
                        pr["repo_full_name"] = repo_full_name

                    batch_prs.append((pr_file, pr_data, output_file))
                    total_valid_prs += 1

                    if len(batch_prs) >= batch_size:
                        stats = {
                            "total_files": total_files,
                            "closed_count": closed_count,
                            "already_processed_count": already_processed_count,
                            "total_valid_prs": total_valid_prs
                        }
                        log_record(f"   Scanned {total_files:,} files, ready to process {len(batch_prs):,} PRs...")
                        yield batch_prs, stats
                        batch_prs = []

                except json.JSONDecodeError as e:
                    log_record(f"Warning: parse failed {pr_file}: {e}")
                except Exception as e:
                    log_record(f"Warning: read failed {pr_file}: {e}")

        if batch_prs:
            stats = {
                "total_files": total_files,
                "closed_count": closed_count,
                "already_processed_count": already_processed_count,
                "total_valid_prs": total_valid_prs
            }
            log_record(f"   Scanned {total_files:,} files, ready to process last {len(batch_prs):,} PRs...")
            yield batch_prs, stats

        log_record(f"Scan done: {total_valid_prs:,} PRs to process")
        log_record(f"   - Total files: {total_files:,}")
        log_record(f"   - Filtered closed: {closed_count:,}")
        log_record(f"   - Already processed (skipped): {already_processed_count:,}")

    except Exception as e:
        log_record(f"Load PR data failed: {e}")
        if batch_prs:
            stats = {
                "total_files": total_files,
                "closed_count": closed_count,
                "already_processed_count": already_processed_count,
                "total_valid_prs": total_valid_prs
            }
            yield batch_prs, stats


def load_prs_from_directory(input_dir: Path, output_dir: Path) -> List[Tuple[Path, Dict, Path]]:
    """Load all PRs from filtered_prs_split (filter closed). Use load_prs_from_directory_batch for large sets."""
    all_prs = []
    for batch_prs, _ in load_prs_from_directory_batch(input_dir, output_dir, batch_size=1000000):
        all_prs.extend(batch_prs)
    return all_prs


def cleanup_closed_prs(output_dir: Path) -> int:
    """Remove closed PR files from output directory. Returns number deleted."""
    if not output_dir.exists():
        return 0

    deleted_count = 0
    scanned_count = 0

    log_record(f"Cleaning closed PR files in output dir...")

    for repo_dir in output_dir.iterdir():
        if not repo_dir.is_dir():
            continue

        for output_file in repo_dir.glob("*.json"):
            scanned_count += 1
            try:
                with output_file.open("r", encoding="utf-8") as f:
                    output_data = json.load(f)
                pr = output_data.get("pr", {})
                pr_status = pr.get("pr_status", "")

                if pr_status == "closed":
                    output_file.unlink()
                    deleted_count += 1
                    if deleted_count % 100 == 0:
                        log_record(f"    Deleted {deleted_count:,} closed PR files...")
            except (json.JSONDecodeError, Exception) as e:
                pass

    log_record(f"Cleanup done: scanned {scanned_count:,} files, deleted {deleted_count:,} closed PRs")
    return deleted_count


def load_prs_from_list_directory(
    list_dir: Path,
    output_dir: Path,
    batch_size: int = 1000,
    target_count: int = 30000,
    directory_json: Path = None
):
    """Load PRs from list directory in batches (generator). Yields (PR list, stats)."""
    if not list_dir.exists() or not list_dir.is_dir():
        log_record(f"PR list directory not found: {list_dir}")
        return

    log_record(f"Loading from PR list dir: {list_dir}")
    log_record(f"Target: {target_count:,} processed PRs in output dir")

    sorted_pr_list = []
    prs_info_dict = {}
    sorted_pr_filenames = []
    use_directory_json = False

    if directory_json and directory_json.exists():
        log_record(f"  Using directory JSON: {directory_json}")
        try:
            with directory_json.open("r", encoding="utf-8") as f:
                directory_data = json.load(f)
            prs_list = directory_data.get("prs", [])
            
            for pr_item in prs_list:
                filename = pr_item.get("filename", "")
                fork_stars = pr_item.get("fork_stars", 0)
                prs_info_dict[filename] = {
                    "repo_full_name": pr_item.get("repo_full_name", ""),
                    "pr_number": pr_item.get("pr_number", 0),
                    "fork_full_name": pr_item.get("fork_full_name", ""),
                    "fork_stars": fork_stars
                }
                sorted_pr_list.append((filename, fork_stars, pr_item))
            
            sorted_pr_list.sort(key=lambda x: x[1], reverse=True)
            sorted_pr_filenames = [item[0] for item in sorted_pr_list]
            use_directory_json = True

            log_record(f"  Read {len(sorted_pr_filenames):,} PRs from directory JSON (by fork stars)")
        except Exception as e:
            log_record(f"Warning: read directory JSON failed: {e}, will use status file")
            use_directory_json = False
    
    status_file = list_dir / "pr_classification_status.json"
    
    existing_output_files = set()
    existing_merged_count = 0
    if output_dir.exists():
        log_record(f"  Scanning output dir for merged PRs...")
        scanned_files = 0
        for repo_dir in output_dir.iterdir():
            if not repo_dir.is_dir():
                continue
            for output_file in repo_dir.glob("*.json"):
                scanned_files += 1
                try:
                    with output_file.open("r", encoding="utf-8") as f:
                        output_data = json.load(f)
                    pr = output_data.get("pr", {})
                    pr_status = pr.get("pr_status", "")

                    if pr_status == "merged":
                        existing_output_files.add(output_file.name)
                        existing_merged_count += 1
                except (json.JSONDecodeError, Exception):
                    pass

                if scanned_files % 1000 == 0:
                    log_record(f"    Scanned {scanned_files:,} files, found {existing_merged_count:,} merged PRs...")

        log_record(f"  Scan done: {scanned_files:,} files, {existing_merged_count:,} merged PRs")

    if existing_merged_count > 0:
        remaining_needed = max(0, target_count - existing_merged_count)
        log_record(f"  Need {remaining_needed:,} more PRs to reach target {target_count:,}")
    else:
        log_record(f"  No merged PRs in output dir; will process until target {target_count:,}")
        remaining_needed = target_count

    if not use_directory_json:
        sorted_pr_filenames = []
        prs_status = {}
        if status_file.exists():
            try:
                with status_file.open("r", encoding="utf-8") as f:
                    status_data = json.load(f)
                prs_status = status_data.get("prs", {})

                if not prs_info_dict:
                    for filename, pr_info in prs_status.items():
                        prs_info_dict[filename] = pr_info

                sorted_pr_items = sorted(
                    prs_status.items(),
                    key=lambda x: x[1].get("fork_stars", 0),
                    reverse=True
                )
                sorted_pr_filenames = [filename for filename, _ in sorted_pr_items]
                log_record(f"  Read {len(sorted_pr_filenames):,} PRs from status file (by fork stars)")
            except Exception as e:
                log_record(f"Warning: read status file failed: {e}, will use filesystem order")
                sorted_pr_filenames = [
                    f.name for f in list_dir.glob("*.json")
                    if f.name != "pr_classification_status.json"
                ]
                sorted_pr_filenames.sort()
        else:
            log_record(f"  Status file not found; using filesystem order")
            sorted_pr_filenames = [
                f.name for f in list_dir.glob("*.json")
                if f.name != "pr_classification_status.json"
            ]
            sorted_pr_filenames.sort()
    
    batch_prs = []
    total_files = 0
    already_classified = 0
    invalid_files = 0
    closed_count = 0
    loaded_count = 0
    
    for pr_filename in sorted_pr_filenames:
        if existing_merged_count + loaded_count >= target_count:
            log_record(f"  Reached target {target_count:,} (current: {existing_merged_count + loaded_count:,}), stop loading")
            break

        total_files += 1
        pr_file = list_dir / pr_filename

        if not pr_file.exists():
            invalid_files += 1
            continue

        if pr_filename in existing_output_files:
            already_classified += 1
            continue
        
        try:
            with pr_file.open("r", encoding="utf-8") as f:
                pr_data = json.load(f)
            
            repo_full_name = pr_data.get("repo_full_name", "unknown")
            pr = pr_data.get("pr", {})

            if not pr:
                invalid_files += 1
                continue

            pr_status_field = pr.get("pr_status", "")
            if pr_status_field == "closed":
                closed_count += 1
                continue

            if repo_full_name == "unknown":
                repo_full_name = pr.get("base_repo_full_name") or pr.get("repo_full_name", "unknown")

            repo_dir_name = sanitize_filename(repo_full_name)
            output_file = output_dir / repo_dir_name / pr_file.name

            if output_file.exists():
                try:
                    with output_file.open("r", encoding="utf-8") as f:
                        output_data = json.load(f)
                    output_pr = output_data.get("pr", {})
                    output_pr_status = output_pr.get("pr_status", "")

                    if output_pr_status == "merged":
                        already_classified += 1
                        existing_output_files.add(pr_filename)
                        continue
                except (json.JSONDecodeError, Exception):
                    pass

            if "repo_full_name" not in pr:
                pr["repo_full_name"] = repo_full_name

            batch_prs.append((pr_file, pr_data, output_file))
            loaded_count += 1

            if len(batch_prs) >= batch_size:
                stats = {
                    "total_files": total_files,
                    "already_classified": already_classified,
                    "invalid_files": invalid_files,
                    "closed_count": closed_count,
                    "loaded_count": loaded_count,
                    "target_count": target_count,
                    "current_count": existing_merged_count + loaded_count
                }
                log_record(f"   Scanned {total_files:,} files, ready {len(batch_prs):,} PRs (merged: {existing_merged_count + loaded_count:,}/{target_count:,})...")
                yield batch_prs, stats
                batch_prs = []

        except json.JSONDecodeError as e:
            log_record(f"Warning: parse failed {pr_file}: {e}")
            invalid_files += 1
        except Exception as e:
            log_record(f"Warning: read failed {pr_file}: {e}")
            invalid_files += 1

    if batch_prs:
        stats = {
            "total_files": total_files,
            "already_classified": already_classified,
            "invalid_files": invalid_files,
            "closed_count": closed_count,
            "loaded_count": loaded_count,
            "target_count": target_count,
            "current_count": existing_merged_count + loaded_count
        }
        log_record(f"   Scanned {total_files:,} files, ready last {len(batch_prs):,} PRs (merged: {existing_merged_count + loaded_count:,}/{target_count:,})...")
        yield batch_prs, stats

    log_record(f"Load from dir done: {loaded_count:,} PRs to process")
    log_record(f"   - Total files: {total_files:,}")
    log_record(f"   - Already classified (skipped): {already_classified:,}")
    log_record(f"   - Invalid: {invalid_files:,}")
    log_record(f"   - Filtered closed: {closed_count:,}")
    log_record(f"   - After processing merged PRs: {existing_merged_count + loaded_count:,}/{target_count:,}")




def load_status_unlocked() -> Dict[str, Any]:
    """Load status file without lock (for use inside save_status)."""
    if not STATUS_FILE.exists():
        return {"processed_files": [], "updated_at": None}
    
    try:
        with STATUS_FILE.open("r", encoding="utf-8") as f:
            data = json.load(f)
            return {
                "processed_files": data.get("processed_files", []),
                "updated_at": data.get("updated_at")
            }
    except Exception:
        return {"processed_files": [], "updated_at": None}


def load_status() -> set:
    """Load set of processed PR file paths (relative) from status file."""
    data = load_status_unlocked()
    processed_files = data.get("processed_files", [])
    return set(processed_files)


def save_status(processed_files: set):
    """Save status file (file lock for multi-process; atomic write)."""
    lock_file = STATUS_FILE.with_suffix(".lock")
    wait_interval = 0.1
    lock_fd = None
    
    try:
        lock_file.parent.mkdir(parents=True, exist_ok=True)
        lock_acquired = False
        
        while not lock_acquired:
            try:
                if sys.platform == "win32" and msvcrt:
                    lock_fd = lock_file.open("wb+")
                    try:
                        msvcrt.locking(lock_fd.fileno(), msvcrt.LK_NBLCK, 1)
                        lock_acquired = True
                    except (IOError, OSError):
                        lock_fd.close()
                        lock_fd = None
                        time.sleep(wait_interval)
                        continue
                else:
                    if fcntl:
                        lock_fd = lock_file.open("w")
                        try:
                            fcntl.flock(lock_fd.fileno(), fcntl.LOCK_EX)
                            lock_acquired = True
                        except (IOError, BlockingIOError, OSError):
                            lock_fd.close()
                            lock_fd = None
                            time.sleep(wait_interval)
                            continue
                    else:
                        lock_acquired = True
            except Exception:
                if lock_fd:
                    try:
                        lock_fd.close()
                    except Exception:
                        pass
                    lock_fd = None
                time.sleep(wait_interval)
                continue
        
        try:
            current_data = load_status_unlocked()
            current_files = set(current_data.get("processed_files", []))
            merged_files = list(current_files | processed_files)
            temp_file = STATUS_FILE.with_suffix(".tmp")
            with temp_file.open("w", encoding="utf-8") as f:
                json.dump({
                    "processed_files": merged_files,
                    "updated_at": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            temp_file.replace(STATUS_FILE)
        finally:
            if lock_fd:
                try:
                    if sys.platform == "win32" and msvcrt:
                        msvcrt.locking(lock_fd.fileno(), msvcrt.LK_UNLCK, 1)
                    elif fcntl:
                        fcntl.flock(lock_fd.fileno(), fcntl.LOCK_UN)
                    lock_fd.close()
                except Exception:
                    pass
            
            try:
                if lock_file.exists():
                    lock_file.unlink()
            except Exception:
                pass

    except Exception as e:
        log_record(f"Warning: save status failed: {e}")
        if lock_fd:
            try:
                lock_fd.close()
            except Exception:
                pass


def get_existing_output_files(output_dir: Path) -> set:
    """Get set of processed PR file paths (relative) from status file."""
    existing_files = load_status()
    if existing_files:
        log_record(f"Read {len(existing_files)} processed PRs from status file")
    else:
        log_record("Status file empty or missing; will process all PRs")
    return existing_files


def build_pr_prompt(pr: Dict, base_prompt: str, categories: List[str]) -> str:
    """Build PR classification prompt (title, body, commit messages)."""
    title = pr.get("title", "") or pr.get("name", "") or "(no title)"
    body = pr.get("body", "") or pr.get("description", "") or ""

    commit_messages = []
    commit_analyses = pr.get("commit_analyses", [])
    commit_messages_from_analyses = []
    if isinstance(commit_analyses, list):
        for commit_analysis in commit_analyses:
            if isinstance(commit_analysis, dict):
                commit_message = commit_analysis.get("commit_message", "")
                if commit_message and commit_message.strip():
                    commit_messages_from_analyses.append(commit_message.strip())
    
    if commit_messages_from_analyses:
        commit_messages = commit_messages_from_analyses
    else:
        commits = pr.get("commits", [])
        if isinstance(commits, list):
            for commit in commits:
                if isinstance(commit, dict):
                    commit_obj = commit.get("commit", {})
                    if commit_obj:
                        commit_message = commit_obj.get("message", "")
                        if commit_message and commit_message.strip():
                            commit_messages.append(commit_message.strip())
    
    commit_messages_text = "\n".join(commit_messages) if commit_messages else "(no commit message)"
    prompt = build_prompt_with_categories(base_prompt, categories)
    prompt = prompt.replace("[PR_TITLE]", title)
    prompt = prompt.replace("[PR_BODY]", body)
    prompt = prompt.replace("[COMMIT_MESSAGES]", commit_messages_text)
    prompt = prompt.replace("[CHANGED_FILES]", "")
    prompt = prompt.replace("[DIFF_SUMMARY_OR_SNIPPETS]", "")
    
    return prompt


async def call_deepseek(prompt: str, pr_id: str, sem: Semaphore, client: httpx.AsyncClient) -> Optional[Dict]:
    """Call DeepSeek API asynchronously (shared HTTP client)."""
    async with sem:
        for attempt in range(RETRY_TIMES):
            try:
                response = await client.post(
                    DEEPSEEK_BASE_URL,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
                    },
                    json={
                        "model": "deepseek-chat",
                        "messages": [
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.3,
                        "max_tokens": 1500
                    },
                    timeout=API_TIMEOUT
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                    
                    try:
                        content = content.strip()
                        if content.startswith("```json"):
                            content = content[7:]
                        if content.startswith("```"):
                            content = content[3:]
                        if content.endswith("```"):
                            content = content[:-3]
                        content = content.strip()
                        
                        classification = json.loads(content)
                        return classification
                    except json.JSONDecodeError as e:
                        log_record(f"Warning: PR {pr_id} API returned non-JSON: {content[:200]}")
                        return None
                else:
                    log_record(f"Warning: PR {pr_id} API failed (status {response.status_code}): {response.text[:200]}")
                    if attempt < RETRY_TIMES - 1:
                        await asyncio.sleep(RETRY_DELAY * (attempt + 1))
                        continue
                    return None
            except Exception as e:
                log_record(f"Warning: PR {pr_id} API error (attempt {attempt + 1}/{RETRY_TIMES}): {e}")
                if attempt < RETRY_TIMES - 1:
                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))
                    continue
                return None
        
        return None


async def classify_pr(pr_data: Dict, output_file: Path, base_prompt: str, categories: List[str], sem: Semaphore, client: httpx.AsyncClient) -> Tuple[Dict, Optional[Dict], Path, Optional[str]]:
    """Classify a single PR. Returns (pr_data, classification, output_file, new_category_if_any)."""
    pr = pr_data.get("pr", {})
    repo_full_name = pr.get("repo_full_name", "") or pr_data.get("repo_full_name", "unknown")
    pr_number = pr.get("number", pr.get("pr_number", ""))
    pr_id = f"{repo_full_name}#{pr_number}"
    
    prompt = build_pr_prompt(pr, base_prompt, categories)
    classification = await call_deepseek(prompt, pr_id, sem, client)
    
    new_category = None
    if classification and isinstance(classification, dict):
        category = classification.get("category")
        if category:
            new_category = category
    
    return (pr_data, classification, output_file, new_category)


async def classify_prs_batch(
    prs: List[Tuple], 
    base_prompt: str, 
    output_dir: Path,
    sem: Semaphore, 
    concurrent_requests: int
) -> int:
    """Classify PRs in batch (concurrent); save each result to output dir. Returns count of successful."""
    categories_lock = asyncio.Lock()
    initial_categories = load_categories()
    log_record(f"Starting concurrent classification of {len(prs)} PRs (concurrency: {concurrent_requests})")
    log_record(f"Initial categories: {len(initial_categories)} ({', '.join(initial_categories[:5])}{'...' if len(initial_categories) > 5 else ''})")

    async with httpx.AsyncClient(
        timeout=API_TIMEOUT,
        limits=httpx.Limits(max_keepalive_connections=concurrent_requests),
        trust_env=False
    ) as client:
        total = len(prs)
        if total == 0:
            return 0

        batch_size = concurrent_requests * 2
        completed = 0
        new_categories_count = 0

        for i in range(0, total, batch_size):
            categories = load_categories()
            batch_prs = prs[i:i + batch_size]
            batch_tasks = []
            batch_files = []
            
            for pr_file_path, pr_data, output_file in batch_prs:
                task = classify_pr(pr_data, output_file, base_prompt, categories, sem, client)
                batch_tasks.append(task)
                batch_files.append(pr_file_path)
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            for pr_file_path, result in zip(batch_files, batch_results):
                if isinstance(result, Exception):
                    log_record(f"Warning: PR {pr_file_path.name} error: {result}")
                elif result:
                    pr_data, classification, output_file, new_category = result

                    if new_category:
                        async with categories_lock:
                            current_categories = load_categories()
                            old_count = len(current_categories)
                            updated_categories = add_category_if_new(new_category, current_categories)
                            if len(updated_categories) > old_count:
                                new_categories_count += 1

                    save_single_pr_result(pr_file_path, pr_data, classification, output_dir)
                    completed += 1

            if completed > 0:
                percentage = completed * 100 // total if total > 0 else 0
                log_record(f"Progress: {completed:,}/{total:,} PRs ({percentage}%), {new_categories_count} new categories")

    if new_categories_count > 0:
        log_record(f"Run found {new_categories_count} new categories; total {len(load_categories())} categories")

    return completed


def export_results_to_csv(output_dir: Path, directory_json: Path = None, csv_file: Path = None):
    """Export classification results to CSV. directory_json used for fork_stars."""
    if csv_file is None:
        csv_file = output_dir.parent / "pr_classification_results.csv"

    log_record(f"Exporting CSV: {csv_file}")

    fork_stars_dict = {}
    if directory_json and directory_json.exists():
        try:
            log_record(f"  Loading directory JSON: {directory_json}")
            with directory_json.open("r", encoding="utf-8") as f:
                directory_data = json.load(f)
            prs_list = directory_data.get("prs", [])
            for pr_item in prs_list:
                filename = pr_item.get("filename", "")
                fork_stars = pr_item.get("fork_stars", 0)
                fork_stars_dict[filename] = fork_stars
            log_record(f"  Loaded fork_stars for {len(fork_stars_dict):,} PRs")
        except Exception as e:
            log_record(f"  Warning: load directory JSON failed: {e}; fork_stars column will be empty")
    
    results = []
    total_files = 0
    processed_files = 0

    log_record(f"  Scanning output dir: {output_dir}")
    for repo_dir in output_dir.iterdir():
        if not repo_dir.is_dir():
            continue
        
        for json_file in repo_dir.glob("*.json"):
            total_files += 1
            try:
                with json_file.open("r", encoding="utf-8") as f:
                    data = json.load(f)
                
                pr = data.get("pr", {})
                classification = data.get("classification")
                
                if not pr or not classification:
                    continue
                
                repo_full_name = data.get("repo_full_name", "") or pr.get("repo_full_name", "") or pr.get("base_repo_full_name", "")
                pr_number = pr.get("number", 0)
                pr_id = f"{repo_full_name}#{pr_number}" if repo_full_name and pr_number else ""
                origin_repo = pr.get("base_repo_full_name", "") or repo_full_name
                fork_repo = pr.get("head_repo_full_name", "")
                pr_url = pr.get("url", "")
                filename = json_file.name
                fork_stars = fork_stars_dict.get(filename, 0)
                pr_title = pr.get("title", "") or pr.get("name", "") or ""
                pr_body = pr.get("body", "") or pr.get("description", "") or ""
                commit_messages = []
                commit_analyses = pr.get("commit_analyses", [])
                commit_messages_from_analyses = []
                if isinstance(commit_analyses, list):
                    for commit_analysis in commit_analyses:
                        if isinstance(commit_analysis, dict):
                            commit_message = commit_analysis.get("commit_message", "")
                            if commit_message and commit_message.strip():
                                commit_messages_from_analyses.append(commit_message.strip())
                
                if commit_messages_from_analyses:
                    commit_messages = commit_messages_from_analyses
                else:
                    commits = pr.get("commits", [])
                    if isinstance(commits, list):
                        for commit in commits:
                            if isinstance(commit, dict):
                                commit_obj = commit.get("commit", {})
                                if commit_obj:
                                    commit_message = commit_obj.get("message", "")
                                    if commit_message and commit_message.strip():
                                        commit_messages.append(commit_message.strip())

                all_commit_messages = "\n".join(commit_messages) if commit_messages else ""
                row_data = {
                    "PR ID": pr_id,
                    "origin repo": origin_repo,
                    "fork repo": fork_repo,
                    "fork stars": fork_stars,
                    "PR number": pr_number,
                    "PR url": pr_url,
                    "PR title": pr_title,
                    "PR body": pr_body,
                    "commit messages": all_commit_messages,
                }
                
                if isinstance(classification, dict):
                    row_data["classification.category"] = classification.get("category", "")
                    row_data["classification.confidence"] = classification.get("confidence", "")
                    row_data["classification.sync_decision"] = classification.get("sync_decision", "")
                    row_data["classification.sync_confidence"] = classification.get("sync_confidence", "")
                    row_data["classification.reason"] = classification.get("reason", "")
                    
                    alt_categories = classification.get("alternative_categories", [])
                    if isinstance(alt_categories, list):
                        alt_categories_str = "; ".join([
                            f"{item.get('category', '')}({item.get('confidence', '')})"
                            if isinstance(item, dict)
                            else str(item)
                            for item in alt_categories
                        ])
                        row_data["classification.alternative_categories"] = alt_categories_str
                    else:
                        row_data["classification.alternative_categories"] = ""
                    
                    evidence = classification.get("evidence", {})
                    if isinstance(evidence, dict):
                        signals = evidence.get("signals", [])
                        row_data["classification.evidence.signals"] = "; ".join(str(s) for s in signals) if isinstance(signals, list) else str(signals)
                        
                        # files
                        files = evidence.get("files", [])
                        row_data["classification.evidence.files"] = "; ".join(str(f) for f in files) if isinstance(files, list) else str(files)
                        
                        # diff_keywords
                        diff_keywords = evidence.get("diff_keywords", [])
                        row_data["classification.evidence.diff_keywords"] = "; ".join(str(k) for k in diff_keywords) if isinstance(diff_keywords, list) else str(diff_keywords)
                    else:
                        row_data["classification.evidence.signals"] = ""
                        row_data["classification.evidence.files"] = ""
                        row_data["classification.evidence.diff_keywords"] = ""
                    
                    for key, value in classification.items():
                        if key not in ["category", "confidence", "sync_decision", "sync_confidence", "reason", "alternative_categories", "evidence"]:
                            if isinstance(value, (dict, list)):
                                row_data[f"classification.{key}"] = json.dumps(value, ensure_ascii=False)
                            else:
                                row_data[f"classification.{key}"] = value
                
                results.append(row_data)
                processed_files += 1
                
                if processed_files % 1000 == 0:
                    log_record(f"    Processed {processed_files:,} files...")

            except json.JSONDecodeError as e:
                log_record(f"  Warning: parse failed {json_file}: {e}")
            except Exception as e:
                log_record(f"  Warning: process failed {json_file}: {e}")

    log_record(f"  Scan done: {total_files:,} files, {processed_files:,} classification results")

    if not results:
        log_record(f"  No classification results; skipping CSV")
        return

    all_columns = set()
    for row in results:
        all_columns.update(row.keys())

    base_columns = ["PR ID", "origin repo", "fork repo", "fork stars", "PR number", "PR url", "PR title", "PR body", "commit messages"]
    classification_columns = sorted([col for col in all_columns if col.startswith("classification.")])
    column_order = base_columns + classification_columns
    
    for col in all_columns:
        if col not in column_order:
            column_order.append(col)

    log_record(f"  Writing CSV: {csv_file}")
    with csv_file.open("w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=column_order)
        writer.writeheader()
        writer.writerows(results)

    log_record(f"  CSV generated: {csv_file}")
    log_record(f"   - {len(results):,} rows")
    log_record(f"   - {len(column_order)} columns")


def save_single_pr_result(
    pr_file_path: Path,
    pr_data: Dict,
    classification: Optional[Dict],
    output_dir: Path
):
    """Save single PR classification result to file; add classification to pr_data; update status file."""
    repo_full_name = pr_data.get("repo_full_name", "unknown")
    pr = pr_data.get("pr", {})
    if not repo_full_name or repo_full_name == "unknown":
        repo_full_name = pr.get("repo_full_name", "unknown")

    output_repo_dir = output_dir / sanitize_filename(repo_full_name)
    output_repo_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_repo_dir / pr_file_path.name

    result_data = pr_data.copy()
    result_data["classification"] = classification
    classification_time = datetime.now().isoformat()
    result_data["classification_time"] = classification_time

    with output_file.open("w", encoding="utf-8") as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)

    output_relative = str(output_file.relative_to(output_dir))
    save_status({output_relative})


async def main():
    """Main entry."""
    parser = argparse.ArgumentParser(description="Classify PRs in filtered_prs_split (merged only).")
    parser.add_argument(
        "--input-dir",
        type=str,
        default=None,
        help=f"Input dir (default: {INPUT_DIR})"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=0,
        help="Max PRs to process this run (0 = no limit)"
    )
    parser.add_argument(
        "--concurrent",
        type=int,
        default=CONCURRENT_REQUESTS,
        help=f"Max concurrent API requests (default: {CONCURRENT_REQUESTS})"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help=f"Output dir (default: {OUTPUT_DIR})"
    )
    parser.add_argument(
        "--pr-list-dir",
        type=str,
        default=None,
        help="PR list dir (if set, read PRs from here instead of input-dir)"
    )
    parser.add_argument(
        "--directory-json",
        type=str,
        default=None,
        help="Directory JSON path (default: pr_classification_directory.json in project root)"
    )
    parser.add_argument(
        "--target-count",
        type=int,
        default=30000,
        help="Target PR count (default: 30000)"
    )
    parser.add_argument(
        "--export-csv",
        action="store_true",
        help="Export classification results to CSV"
    )
    parser.add_argument(
        "--csv-file",
        type=str,
        default=None,
        help="CSV output path (default: pr_classification_results.csv in output parent)"
    )
    args = parser.parse_args()
    
    input_dir = Path(args.input_dir) if args.input_dir else INPUT_DIR
    if args.input_dir and Path(args.input_dir).is_absolute():
        input_dir = Path(args.input_dir)
    elif args.input_dir:
        input_dir = CURRENT_DIR / args.input_dir
    
    output_dir = Path(args.output_dir) if args.output_dir else OUTPUT_DIR
    if args.output_dir and Path(args.output_dir).is_absolute():
        output_dir = Path(args.output_dir)
    elif args.output_dir:
        output_dir = CURRENT_DIR / args.output_dir
    
    list_dir = None
    if args.pr_list_dir:
        list_dir = Path(args.pr_list_dir) if Path(args.pr_list_dir).is_absolute() else CURRENT_DIR / args.pr_list_dir
    
    directory_json = None
    if args.directory_json:
        directory_json = Path(args.directory_json) if Path(args.directory_json).is_absolute() else CURRENT_DIR / args.directory_json
    else:
        default_directory_json = CURRENT_DIR / "pr_classification_directory.json"
        if default_directory_json.exists():
            directory_json = default_directory_json
    
    use_pr_list_dir = list_dir.exists() if list_dir else False
    concurrent_requests = args.concurrent
    max_prs = args.limit
    target_count = args.target_count

    log_record("=== PR classification ===")
    if max_prs > 0:
        log_record(f"Limit: {max_prs} PRs this run")
    log_record(f"Concurrency: {concurrent_requests}")
    if use_pr_list_dir:
        log_record(f"PR list dir: {list_dir}")
        if directory_json and directory_json.exists():
            log_record(f"Using directory JSON: {directory_json}")
            log_record(f"   - Process by fork stars; skip already processed")
        else:
            log_record(f"Directory JSON not found; will use status file if present")
    else:
        log_record(f"Scan dir: {input_dir}")

    base_prompt = load_prompt()
    if not base_prompt:
        log_record("Cannot load prompt; exiting")
        return

    output_dir.mkdir(parents=True, exist_ok=True)
    log_record(f"Output dir: {output_dir}")

    if use_pr_list_dir:
        deleted_count = cleanup_closed_prs(output_dir)
        if deleted_count > 0:
            log_record(f"   Cleaned {deleted_count:,} closed PR files")
        log_record("")

    sem = Semaphore(concurrent_requests)
    log_record("Starting scan-and-process PR...")
    
    total_completed = 0
    total_processed_batches = 0
    batch_scan_size = 1000
    has_any_prs = False

    try:
        if use_pr_list_dir:
            for batch_prs, stats in load_prs_from_list_directory(list_dir, output_dir, batch_size=batch_scan_size, target_count=target_count, directory_json=directory_json):
                if not batch_prs:
                    continue

                has_any_prs = True

                if max_prs > 0:
                    remaining = max_prs - total_completed
                    if remaining <= 0:
                        log_record(f"Reached limit {max_prs}; stopping")
                        break
                    if len(batch_prs) > remaining:
                        batch_prs = batch_prs[:remaining]
                        log_record(f"Limit: processing only {len(batch_prs)} PRs this batch (limit {max_prs})")

                total_processed_batches += 1
                log_record(f"Batch {total_processed_batches}: processing {len(batch_prs):,} PRs...")

                batch_completed = await classify_prs_batch(batch_prs, base_prompt, output_dir, sem, concurrent_requests)
                total_completed += batch_completed

                log_record(f"Batch {total_processed_batches} done: {batch_completed:,} PRs, total {total_completed:,}")

                if max_prs > 0 and total_completed >= max_prs:
                    log_record(f"Reached limit {max_prs}; stopping")
                    break
        else:
            for batch_prs, stats in load_prs_from_directory_batch(input_dir, output_dir, batch_size=batch_scan_size):
                if not batch_prs:
                    continue

                has_any_prs = True

                if max_prs > 0:
                    remaining = max_prs - total_completed
                    if remaining <= 0:
                        log_record(f"Reached limit {max_prs}; stopping")
                        break
                    if len(batch_prs) > remaining:
                        batch_prs = batch_prs[:remaining]
                        log_record(f"Limit: processing only {len(batch_prs)} PRs this batch (limit {max_prs})")

                total_processed_batches += 1
                log_record(f"Batch {total_processed_batches}: processing {len(batch_prs):,} PRs...")

                batch_completed = await classify_prs_batch(batch_prs, base_prompt, output_dir, sem, concurrent_requests)
                total_completed += batch_completed

                log_record(f"Batch {total_processed_batches} done: {batch_completed:,} PRs, total {total_completed:,}")

                if max_prs > 0 and total_completed >= max_prs:
                    log_record(f"Reached limit {max_prs}; stopping")
                    break

        completed_count = total_completed

        if not has_any_prs:
            log_record("No PRs to process (all done or closed).")
            if args.export_csv:
                csv_file_path = None
                if args.csv_file:
                    csv_file_path = Path(args.csv_file) if Path(args.csv_file).is_absolute() else CURRENT_DIR / args.csv_file
                export_results_to_csv(output_dir, directory_json, csv_file_path)
            return

        log_record(f"Done: processed {completed_count:,} PRs")
        log_record(f"   - Output dir: {output_dir}")
        log_record(f"   - Each PR saved as separate file")
        log_record(f"   - Resume: next run will skip already processed PRs")

        if args.export_csv:
            csv_file_path = None
            if args.csv_file:
                csv_file_path = Path(args.csv_file) if Path(args.csv_file).is_absolute() else CURRENT_DIR / args.csv_file
            export_results_to_csv(output_dir, directory_json, csv_file_path)
    except KeyboardInterrupt:
        log_record("\nInterrupted; processed PRs saved to output dir")
        log_record("   - Next run will skip existing output files")


if __name__ == "__main__":
    asyncio.run(main())

